{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDH3w9fzg5MunqVnIh6o8m",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alhichri/BlindSys/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qcIHdHbL3Kc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras_applications"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeLG5b_UIKds",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "import numpy as np\n",
        "import itertools\n",
        "import scipy.io\n",
        "import matplotlib.pyplot as plt\n",
        "from pickle import load, dump\n",
        "import h5py\n",
        "import time\n",
        "\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_last')\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.preprocessing.image import load_img\n",
        "from keras.preprocessing.image import img_to_array\n",
        "#from keras.applications.vgg16 import preprocess_input\n",
        "from keras.applications import inception_v3\n",
        "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Activation, BatchNormalization, Conv1D,GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import ReLU, LeakyReLU, PReLU, ELU, Conv2D, MaxPooling2D, Flatten\n",
        "from keras.layers.merge import concatenate, add, maximum\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import plot_model\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix, classification_report\n",
        "from sklearn.metrics import hamming_loss, label_ranking_loss, zero_one_loss\n",
        "from sklearn.utils.multiclass import unique_labels\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive' , force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6i_XqfPJOmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from importlib.machinery import SourceFileLoader\n",
        "somemodule = SourceFileLoader('squeezenet', '/content/gdrive/My Drive/000papers/utils/squeezenet.py').load_module()\n",
        "from squeezenet import SqueezeNet\n",
        "#DNet1 = SqueezeNet(input_shape=img_dim, include_top=True,weights='imagenet')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAJrHJwjI9g2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n",
        "    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n",
        "    acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' not in s]\n",
        "    val_acc_list = [s for s in history.history.keys() if 'acc' in s and 'val' in s]\n",
        "    \n",
        "    if len(loss_list) == 0:\n",
        "        print('Loss is missing in history')\n",
        "        return \n",
        "    \n",
        "    ## As loss always exists\n",
        "    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n",
        "    \n",
        "    ## Loss\n",
        "    plt.figure(1)\n",
        "    for l in loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    for l in val_loss_list:\n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n",
        "    \n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    \n",
        "    ## Accuracy\n",
        "    plt.figure(2)\n",
        "    for l in acc_list:\n",
        "        plt.plot(epochs, history.history[l], 'b', label='Training accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "    for l in val_acc_list:    \n",
        "        plt.plot(epochs, history.history[l], 'g', label='Validation accuracy (' + str(format(history.history[l][-1],'.5f'))+')')\n",
        "\n",
        "    plt.title('Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_confusion_matrix2(cm, title='Confusion matrix', cmap=plt.cm.Oranges):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(cm.shape[1])\n",
        "    plt.xticks(tick_marks, rotation=45)\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticklabels((ax.get_xticks() +1).astype(str))\n",
        "    plt.yticks(tick_marks)\n",
        "    for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +  ax.get_xticklabels() + ax.get_yticklabels()):\n",
        "        item.set_fontsize(20)\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], 'd'),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\",  fontsize=28, rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "\n",
        "\n",
        "def Evaluate(ACTUAL,PREDICTED):\n",
        "#% This fucntion evaluates the performance of a classification model by \n",
        "#% calculating the common performance measures: Accuracy, Sensitivity, \n",
        "#% Specificity, Precision, Recall, F-Measure, G-mean.\n",
        "#% Input: ACTUAL = Column matrix with actual class labels of the training\n",
        "#%                 examples\n",
        "#%        PREDICTED = Column matrix with predicted class labels by the\n",
        "#%                    classification model\n",
        "#% Output: EVAL = Row matrix with all the performance measures\n",
        "\n",
        "    idx = ACTUAL==1\n",
        "    p = len(ACTUAL[idx])\n",
        "    n = len(ACTUAL[~idx])\n",
        "    N = p+n;\n",
        "    tp = np.sum(ACTUAL[idx]==PREDICTED[idx])\n",
        "    tn = np.sum(ACTUAL[~idx]==PREDICTED[~idx])\n",
        "    fp = n-tn\n",
        "    fn = p-tp\n",
        "    tp_rate = tp/p\n",
        "    tn_rate = tn/n\n",
        "    \n",
        "    accuracy = (tp+tn)/N\n",
        "    sensitivity = tp_rate\n",
        "    specificity = tn_rate\n",
        "    precision = tp/(tp+fp)\n",
        "    recall = sensitivity\n",
        "    f_measure = 2*((precision*recall)/(precision + recall))\n",
        "    gmean = np.sqrt(tp_rate*tn_rate)\n",
        "    HL = hamming_loss(ACTUAL,PREDICTED)\n",
        "    ZeroOneLoss = zero_one_loss(ACTUAL,PREDICTED)\n",
        "    #print('ZeroOneLoss: ', ZeroOneLoss)\n",
        "\n",
        "    #disp('      accuracy  | sensitivity | specificity | precision |  recall  |  f_measure  |   gmean')\n",
        "    EVAL = np.empty(9,)\n",
        "    EVAL[0] = accuracy; EVAL[1] = sensitivity; EVAL[2] = specificity;\n",
        "    EVAL[3] = precision; EVAL[4] = recall; EVAL[5] = f_measure; EVAL[6] = gmean; \n",
        "    EVAL[7] = HL;  EVAL[8] = ZeroOneLoss\n",
        "    EVAL = 100*EVAL\n",
        "\n",
        "    return EVAL\n",
        "\n",
        "\n",
        "def Evaluate_Multilabel_classification(ACTUAL,PREDICTED, number_of_labels):\n",
        "    Complete_Evaluation = np.zeros((number_of_labels , 9))\n",
        "    Complete_Evaluation[0,:] = Evaluate(ACTUAL[:,0], PREDICTED[:,0])\n",
        "    Summary_Evaluation = (1/number_of_labels)*Complete_Evaluation[0,:]\n",
        "    for i in range(1,number_of_labels):\n",
        "        Complete_Evaluation[i,:] = Evaluate(ACTUAL[:,i], PREDICTED[:,i])\n",
        "        Summary_Evaluation = Summary_Evaluation + (1/number_of_labels)*Complete_Evaluation[i,:]\n",
        "    RankingLoss = label_ranking_loss(ACTUAL,PREDICTED)\n",
        "\n",
        "\n",
        "    return Complete_Evaluation, Summary_Evaluation\n",
        "\n",
        "def trainModel(VGG16Model,Xtrain, ytrain, learningRate=0.001,epochs=100, batch_size=32):\n",
        "        VGG16Model.compile(loss=['mse'], optimizer=Adam(lr= learningRate ), metrics=['mse','mae'])\n",
        "        Loss = []\n",
        "        for epoch in range(epochs):\n",
        "            t1 = time.time()\n",
        "            if (epoch+1) % 100 == 0:\n",
        "                learningRate = learningRate * 0.1\n",
        "                VGG16Model.compile(loss=['mse'], optimizer=Adam(lr=learningRate),metrics=['mse','mae'])\n",
        "            print('Epoch {} of {}'.format(epoch + 1, epochs))\n",
        "            n_batches = num_train_samples / batch_size\n",
        "            # print(n_batches)\n",
        "            idx_train_shuffle_1 = np.arange(Xtrain.shape[0])\n",
        "            np.random.shuffle(idx_train_shuffle_1)\n",
        "            epoch_loss = []\n",
        "            # epoch_t2_loss = []\n",
        "            index = 0\n",
        "            while index < n_batches :\n",
        "                t1_data = Xtrain[idx_train_shuffle_1[index * batch_size:(index + 1) * batch_size]]\n",
        "                t1_label = ytrain[idx_train_shuffle_1[index * batch_size:(index + 1) * batch_size]]\n",
        "                if t1_label.shape[0] < 1:\n",
        "                    idx = np.random.randint(0, Xtrain.shape[0], batch_size)\n",
        "                    t1_data = Xtrain[idx]\n",
        "                    t1_label = ytrain[idx]\n",
        "                Loss1 = VGG16Model.train_on_batch(t1_data, t1_label)\n",
        "                epoch_loss.append(Loss1[0])\n",
        "                index += 1\n",
        "            print('\\n[Loss : {:.3f} ] Time'.format(np.mean(epoch_loss)), time.time() - t1)\n",
        "            Loss.append(np.mean(epoch_loss))\n",
        "        print('\\n[Loss : {:.3f} ] Time'.format(np.mean(epoch_loss)), time.time() - t0)\n",
        "        return VGG16Model, np.array(Loss)\n",
        "\n",
        "\n",
        "def FitModel(The_model, file_name, Xtrain, ytrain, learningRate, epochs, batch_size ):\n",
        "\n",
        "    The_model.compile(loss=['mse'], optimizer=Adam(lr=0.001), metrics=['mse','mae'])\n",
        "    model_history0 = The_model.fit(Xtrain, ytrain, \n",
        "                                batch_size=batch_size, \n",
        "                                epochs=80, \n",
        "                                validation_split=0.2,\n",
        "                                verbose=1, shuffle=True)    \n",
        "    The_model.compile(loss=['mse'], optimizer=Adam(lr=0.0001), metrics=['mse','mae'])\n",
        "    \n",
        "    mc = ModelCheckpoint(file_name, monitor='mse', \n",
        "                         mode='min', verbose=1, \n",
        "                         save_best_only=True)\n",
        "    # patient early stopping\n",
        "    es=EarlyStopping(monitor='loss', min_delta=0.00005, patience=20, \\\n",
        "                          verbose=1, mode='auto')\n",
        "    model_history1 = The_model.fit(Xtrain, ytrain, \n",
        "                                batch_size=batch_size, \n",
        "                                epochs=40, \n",
        "                                verbose=1, shuffle=True,\n",
        "                                callbacks=[es,mc])                                \n",
        "    model_history0.history['loss'] = model_history0.history['loss'] +  model_history1.history['loss']  \n",
        "    #model_history0.history['acc'] = model_history0.history['acc'] +  model_history1.history['acc']  \n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5L0aPTxnMq_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = 0\n",
        "if dataset==0:\n",
        "    path = '/content/gdrive/My Drive/Multi_objects_datasets/'\n",
        "    mat = scipy.io.loadmat(path+'ksu1.mat')\n",
        "    X = mat['trainall'][0];  Y = mat['testall'][0]\n",
        "    mat = scipy.io.loadmat(path+'ksu1Labels.mat')\n",
        "    L_train = mat['Labels_training']; L_test = mat['Labels_test'];  Names = mat['names']\n",
        "    num_samples = len(X)\n",
        "    object_set = ['Pillar', 'Fire extinguisher/hose', 'Trash can', 'Chairs', 'External Door', 'Hallway', 'Self-service', 'Reception', 'Didactic service machine', 'Display Screen', 'Board', 'Stairs', 'Elevator', 'Laboratory', 'Internal Door']\n",
        "elif dataset == 1:\n",
        "    path = '/content/gdrive/My Drive/Multi_objects_datasets/'\n",
        "    mat = scipy.io.loadmat(path+'ksu2.mat')\n",
        "    X = mat['trainall'][0];  Y = mat['testall'][0]\n",
        "    mat = scipy.io.loadmat(path+'ksu2Labels.mat')\n",
        "    L_train = mat['Labels_training']; L_test = mat['Labels_test'];  Names = mat['names']\n",
        "    num_samples = len(X)\n",
        "    object_set = ['Board', 'Fire extinguisher/hose', 'Trash cans', 'Chairs', 'External door', 'didactic service machine', 'Self-service', 'Reception', 'Cafeteria', 'Display screen', 'Pillar', 'Stairs', 'Elevator', 'Prayer room', 'Internal door']\n",
        "elif dataset == 2:\n",
        "    path = '/content/gdrive/My Drive/Multi_objects_datasets/'\n",
        "    mat = scipy.io.loadmat(path+'UTrento1.mat')\n",
        "    X = mat['trainall'][0];  Y = mat['testall'][0]\n",
        "    mat = scipy.io.loadmat(path+'UTrento1Labels.mat')\n",
        "    L_train = mat['Labels_training']; L_test = mat['Labels_test'];  Names = mat['names']\n",
        "    num_samples = len(X)\n",
        "    object_set = ['External Window', 'Board', 'Table', 'External Door', 'Stair Door', 'Access Control Reader', 'Ofﬁce', 'Pillar', 'Display Screen', 'People', 'ATM', 'Chairs', 'Bins', 'Internal Door',  'Elevator']\n",
        "elif dataset==3:\n",
        "    path = '/content/gdrive/My Drive/Multi_objects_datasets/'\n",
        "    mat = scipy.io.loadmat(path+'UTrento2.mat')\n",
        "    X = mat['trainall'][0];  Y = mat['testall'][0]\n",
        "    mat = scipy.io.loadmat(path+'UTrento2Labels.mat')\n",
        "    L_train = mat['Labels_training']; L_test = mat['Labels_test'];  Names = mat['names']\n",
        "    num_samples = len(X)\n",
        "    object_set = ['Stairs', 'Heater', 'Corridor', 'Board', 'Laboratories', 'Bins', 'Ofﬁce', 'People', 'Pillar', 'Elevator', 'Reception', 'Chairs', 'Self-Service', 'External Door', 'Display Screen']    \n",
        "\n",
        "Xtrain = np.stack(X)\n",
        "Xtest = np.stack(Y)\n",
        "\n",
        "# process inputs, removing mean\n",
        "# must cionvert each image to arrauy first : test_img = np.array(test_img)\n",
        "#X = preprocess_input(np.float6a4(X))\n",
        "#X = X/255.0\n",
        "X = preprocess_input(np.float64(Xtrain))\n",
        "Xtest = preprocess_input(np.float64(Xtest))\n",
        "\n",
        "ytrain = L_train.T; ytrain = ytrain.astype(int)\n",
        "ytest = L_test.T; ytest = ytest.astype(int)\n",
        "\n",
        "# make the number of traning samples a multiple of 32\n",
        "num_classes = len(Names)\n",
        "num_train_samples = len(Xtrain)\n",
        "temp = int(np.floor(num_train_samples/32)+1)\n",
        "new_train_size = temp*32\n",
        "extra_samples = new_train_size - num_train_samples\n",
        "Xtrain = np.vstack((X, X[:extra_samples] ))\n",
        "ytrain = np.vstack((ytrain , ytrain[:extra_samples]))\n",
        "\n",
        "num_train_samples = len(Xtrain)\n",
        "\n",
        "img_rows, img_cols, img_channels = Xtrain[0].shape[0], Xtrain[0].shape[1], Xtrain[0].shape[2]\n",
        "# Parameters for the DenseNet model builder\n",
        "(img_channels, img_rows, img_cols) if K.image_data_format() == 'channels_first' else (\n",
        "img_rows, img_cols, img_channels)\n",
        "\n",
        "print('Xtrain shape: ', Xtrain.shape)\n",
        "print('ytrain shape: ', ytrain.shape)\n",
        "print('Xtest shape: ', Xtest.shape)\n",
        "print('ytest shape: ', ytest.shape)\n",
        "\n",
        "frequency_of_objects = np.sum(ytrain, axis = 0)\n",
        "for i in range(num_classes):\n",
        "    print('Object: ', i, ' number of occurences: ' , frequency_of_objects[i]  )\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL6WRhscNgb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "range_0_to_14 = np.expand_dims (  np.arange(0,15,1).T  , axis=1)\n",
        "batch_size = 32 #int(np.floor(num_train_samples/4)+2) #np.min([100 , 0.4*num_train_samples]).astype(int)      \n",
        "k = 0;  dataset_OA = np.zeros((1,10)) ; times_arr = [] \n",
        "while k<1:  \n",
        "\n",
        "################################ squeezeNet model   ################################\n",
        "    img_dim = [img_rows,img_cols,img_channels]\n",
        "    DNet1 = SqueezeNet(input_shape=img_dim, include_top=True,weights='imagenet')\n",
        "    #DNet1.summary()\n",
        "    Net_layers=len(DNet1.layers)\n",
        "    DNet1.trainable=False\n",
        "    layer_name = 'fire9/concat'\n",
        "    x = DNet1.get_layer(layer_name).output\n",
        "    x = Conv2D(filters=256 , kernel_size=(1,1) , strides=(1,1))(x)\n",
        "    x = LeakyReLU(alpha=0.5)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    last_layer = GlobalAveragePooling2D()(x)\n",
        "    classifier2 = Dense(num_classes, activation='linear')(last_layer)\n",
        "    SQNetModel = Model(inputs=DNet1.inputs, outputs=classifier2)\n",
        "    SQNetModel.summary()\n",
        "    #raise ValueError(\"Script End\")\n",
        "    \n",
        "    DNet2 = SqueezeNet(input_shape=img_dim, include_top=True,weights='imagenet')\n",
        "    #DNet1.summary()\n",
        "    Net_layers=len(DNet2.layers)\n",
        "    DNet2.trainable=False\n",
        "    layer_name = 'fire9/concat'\n",
        "    x = DNet2.get_layer(layer_name).output\n",
        "    x = Conv2D(filters=256 , kernel_size=(1,1) , strides=(1,1))(x)\n",
        "    x = LeakyReLU(alpha=0.5)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    last_layer = GlobalAveragePooling2D()(x)\n",
        "    classifier2 = Dense(num_classes, activation='linear')(last_layer)\n",
        "    SQNetModel4error = Model(inputs=DNet2.inputs, outputs=classifier2)\n",
        "    SQNetModel4error.summary()\n",
        "\n",
        "    t0 = time.time();                  \n",
        "    file_name = 'SQNetModel_best.h5'\n",
        "    SQNetModel,model_history1_0 = trainModel(SQNetModel , Xtrain, ytrain, learningRate=0.001 , epochs=2, batch_size=32 )\n",
        "    plt.plot(model_history1_0)\n",
        "#            SQNetModel,model_history1_0 = FitModel(SQNetModel, file_name , Xtrain, ytrain, learningRate=0.001 , epochs=60, batch_size=32 )\n",
        "#            plot_history(model_history1_0) \n",
        "\n",
        "\n",
        "\n",
        "    Xtrain_outputs1 = SQNetModel.predict(Xtrain, batch_size=32)\n",
        "    Xtrain_errors1 = Xtrain_outputs1 - ytrain\n",
        "    file_name = 'SQNetModel4error_best.h5'\n",
        "    SQNetModel4error,model_history1_1 = trainModel(SQNetModel4error , Xtrain, Xtrain_errors1, learningRate=0.001 , epochs=2, batch_size=32 )\n",
        "    plt.plot(model_history1_1)\n",
        "#            SQNetModel4error,model_history1_1 = FitModel(SQNetModel, file_name , Xtrain, errorInPrediction, learningRate=0.001 , epochs=30, batch_size=32 )\n",
        "#            plot_history(model_history1_1)           \n",
        "    time_for_model1 = time.time() -t0\n",
        "\n",
        "    Prob_task1 = SQNetModel.predict(Xtest, batch_size=32)           \n",
        "    errorPrediction1 = SQNetModel4error.predict(Xtest, batch_size=32)\n",
        "    mseErrorPredictionSqueezeNet = mean_squared_error(errorPrediction1,Prob_task1-ytest)\n",
        "\n",
        "\n",
        "################################ VGG16 model   ################################\n",
        "    img_dim = (img_rows,img_cols,img_channels)\n",
        "    DNet3=VGG16(input_shape=img_dim,include_top=False, weights='imagenet', classes=1000)\n",
        "    #DNet1.summary()\n",
        "    Net_layers=len(DNet3.layers)\n",
        "    DNet3.trainable=False\n",
        "    for layer in range(Net_layers-2):\n",
        "          DNet3.layers[layer].trainable = False\n",
        "\n",
        "    x = DNet3.output  # check which one is better??\n",
        "    #x = BatchNormalization()(feature_tensor)\n",
        "    feature_tensor = GlobalAveragePooling2D(name='branch1_02')(x)\n",
        "    x = Dense(128)(feature_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    last_layer = LeakyReLU(0.5)(x)\n",
        "    classifier2 = Dense(num_classes, activation='linear')(last_layer)\n",
        "\n",
        "\n",
        "    VGG16Model = Model(inputs=DNet3.inputs, outputs=classifier2)\n",
        "    VGG16Model.summary()\n",
        "\n",
        "    img_dim = (img_rows,img_cols,img_channels)\n",
        "    DNet4=VGG16(input_shape=img_dim,include_top=False, weights='imagenet', classes=1000)\n",
        "    #DNet1.summary()\n",
        "    Net_layers=len(DNet4.layers)\n",
        "    DNet1.trainable=False\n",
        "    for layer in range(Net_layers-2):\n",
        "          DNet4.layers[layer].trainable = False\n",
        "    x = DNet4.output  # check which one is better??\n",
        "    #x = BatchNormalization()(feature_tensor)\n",
        "    feature_tensor = GlobalAveragePooling2D(name='branch1_02')(x)\n",
        "    x = Dense(128)(feature_tensor)\n",
        "    x = BatchNormalization()(x)\n",
        "    last_layer = LeakyReLU(0.5)(x)\n",
        "    classifier2 = Dense(num_classes, activation='linear')(last_layer)\n",
        "    VGG16Model4error = Model(inputs=DNet4.inputs, outputs=classifier2)\n",
        "    VGG16Model4error.summary()\n",
        "\n",
        "    t0 = time.time();    batch_size = 16               \n",
        "    VGG16Model,model_history2_0 = trainModel(VGG16Model , Xtrain, ytrain, learningRate=0.001 , epochs=2, batch_size=16 )\n",
        "    plt.plot(model_history2_0)      \n",
        "\n",
        "    Xtrain_outputs2 = VGG16Model.predict(Xtrain, batch_size=32)\n",
        "    Xtrain_errors2 = Xtrain_outputs2 - ytrain\n",
        "    VGG16Model4error,model_history2_1 = trainModel(VGG16Model4error , Xtrain, Xtrain_errors2, learningRate=0.001 , epochs=2, batch_size=16 )\n",
        "    plt.plot(model_history2_1)\n",
        "\n",
        "    time_for_model2 = time.time() -t0\n",
        "\n",
        "\n",
        "    #This works but I don't know what to do with it:\n",
        "    # multilabel_confusion_matrix(ytest,Test_predicted)\n",
        "    # Carry out classification\n",
        "    t3 = time.time()\n",
        "    Prob_task1 = SQNetModel.predict(Xtest, batch_size=32)           \n",
        "    errorPrediction1 = SQNetModel4error.predict(Xtest, batch_size=32)\n",
        "    mseErrorPredictionSqueezeNet = mean_squared_error(errorPrediction1,Prob_task1-ytest)\n",
        "    del SQNetModel, SQNetModel4error\n",
        "    Prob_task2 = VGG16Model.predict(Xtest, batch_size=32)\n",
        "    errorPrediction2 = VGG16Model4error.predict(Xtest, batch_size=32)\n",
        "    mseErrorPredictionVGG16 = mean_squared_error(errorPrediction2,Prob_task2-ytest)\n",
        "\n",
        "    time_per_image = (time.time() - t3) / len(Xtest)\n",
        "    times_arr = np.hstack((times_arr , time_per_image))\n",
        "\n",
        "    #del VGG16Model, VGG16Model4error\n",
        "\n",
        "    ###################  Save current results ##############################\n",
        "\n",
        "    # runNumber = dataset; path = ''\n",
        "    # fname = path + 'predictionresults%i.npz' % runNumber\n",
        "    # D = {'Prob_task1': Prob_task1, 'Prob_task2': Prob_task2 , \n",
        "    #       'errorPrediction1': errorPrediction1, 'errorPrediction2': errorPrediction2}\n",
        "    # np.savez( fname , **D)\n",
        "    # print('Saving prediction results in file called predictionresults%i.npz' % runNumber)\n",
        "\n",
        "\n",
        "    ###################  load and fuse ##############################\n",
        "    ###################  load and fuse ##############################\n",
        "\n",
        "    # runNumber = 1; path=''\n",
        "    # fname = path + 'predictionresults%i.npz' % runNumber\n",
        "    # D = np.load(fname)\n",
        "    # Prob_task1 = D['Prob_task1']; \n",
        "    # Prob_task2 = D['Prob_task2']; \n",
        "    # errorPrediction1 = D['errorPrediction1']; \n",
        "    # errorPrediction2 = D['errorPrediction2']; \n",
        "\n",
        "\n",
        "    ###################  Fuse Results using OWA ##############################\n",
        "\n",
        "    Prob_task = Prob_task1.copy()\n",
        "    for obj in range(15):\n",
        "        #print(obj)\n",
        "        e1 = 1 /(1+np.abs( errorPrediction1[:,obj] ) )\n",
        "        e2 = 1 / (1+np.abs( errorPrediction2[:,obj]  )  )\n",
        "        w1 = e1.copy(); w2 = e2.copy()\n",
        "        for sample in range(len(e1)):\n",
        "            if e2[sample]>e1[sample]:\n",
        "                w2[sample] = 1/(1+e2[sample])\n",
        "                w1[sample] =  e2[sample] / (1+e2[sample])\n",
        "            else:\n",
        "                w1[sample] = 1/(1+e1[sample])\n",
        "                w2[sample] =  e1[sample] / (1+e1[sample])   \n",
        "#                w1 = 1 - e1 / (e1+e2)\n",
        "#                w2 = 1 - e2 / (e1+e2)\n",
        "        Prob_task[:,obj] = w1 * Prob_task1[:,obj] + w2* Prob_task2[:,obj]  \n",
        "\n",
        "\n",
        "    bestThresh = 0.3\n",
        "    AVG1 = np.zeros((  8 ))\n",
        "    print('accuracy  | sensitivity | specificity | precision |  recall  |  f_measure  |   gmean')\n",
        "    for i in (1,2,3,4,5,6,7,8):\n",
        "        Test_predicted1 = Prob_task1 > i*0.1;   Test_predicted1 =Test_predicted1 *1\n",
        "        Complete_Evaluation1, Summary_Evaluation1 = Evaluate_Multilabel_classification(ytest,Test_predicted1, num_classes)\n",
        "        AVG1[i-1] = ( (Summary_Evaluation1[1] + Summary_Evaluation1[2])/2 )\n",
        "    Test_predicted1 = Prob_task1 > bestThresh;   Test_predicted1 =Test_predicted1 *1\n",
        "    Complete_Evaluation1, Summary_Evaluation1 = Evaluate_Multilabel_classification(ytest,Test_predicted1, num_classes)\n",
        "    dataset_OA = np.vstack((dataset_OA , np.hstack(((Summary_Evaluation1[1] + Summary_Evaluation1[2])/2 , Summary_Evaluation1))   ))\n",
        "\n",
        "    AVG2 = np.zeros((  8 ))\n",
        "    for i in (1,2,3,4,5,6,7,8):\n",
        "        Test_predicted2 = Prob_task2 > i*0.1;   Test_predicted2 =Test_predicted2 *1\n",
        "        Complete_Evaluation2, Summary_Evaluation2 = Evaluate_Multilabel_classification(ytest,Test_predicted2, num_classes)\n",
        "        AVG2[i-1] = ( (Summary_Evaluation2[1] + Summary_Evaluation2[2])/2 )\n",
        "    Test_predicted2 = Prob_task2 > bestThresh;   Test_predicted2 =Test_predicted2 *1\n",
        "    Complete_Evaluation2, Summary_Evaluation2 = Evaluate_Multilabel_classification(ytest,Test_predicted2, num_classes)\n",
        "    dataset_OA = np.vstack((dataset_OA , np.hstack(((Summary_Evaluation2[1] + Summary_Evaluation2[2])/2 , Summary_Evaluation2))   ))\n",
        "\n",
        "    AVG3 = np.zeros((  8 ))\n",
        "    for i in (1,2,3,4,5,6,7,8):\n",
        "        Test_predicted3 = Prob_task > i*0.1;   Test_predicted3 =Test_predicted3 *1\n",
        "        Complete_Evaluation3, Summary_Evaluation3 = Evaluate_Multilabel_classification(ytest,Test_predicted3, num_classes)\n",
        "        AVG3[i-1] = ( (Summary_Evaluation3[1] + Summary_Evaluation3[2])/2 )\n",
        "    Test_predicted3 = Prob_task > bestThresh;   Test_predicted3 =Test_predicted3 *1\n",
        "    Complete_Evaluation3, Summary_Evaluation3 = Evaluate_Multilabel_classification(ytest,Test_predicted3, num_classes)\n",
        "    dataset_OA = np.vstack((dataset_OA , np.hstack(((Summary_Evaluation3[1] + Summary_Evaluation3[2])/2 , Summary_Evaluation3))   ))\n",
        "\n",
        "    k = k +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nn20GJtRm2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8], AVG1, '--b', label='SqueezeNet')\n",
        "ax.plot([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8], AVG2, '-r', label='VGG16')\n",
        "ax.plot([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8], AVG3, '-ok', label='Fused')\n",
        "#ax.axis('equal')\n",
        "plt.xlabel('Threshold'); plt.ylabel('AVG(SEN+SPE)')\n",
        "leg = ax.legend();\n",
        "ax.legend(loc='upper right', frameon=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHvqa6uTOaYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}